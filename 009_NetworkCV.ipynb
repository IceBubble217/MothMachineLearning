{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callin Switzer\n",
    "### 9 Nov 2018\n",
    "___\n",
    "### - Train Dense, Feedforward Neural Network with Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\calli\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow successfully installed.\n",
      "The installed version of TensorFlow includes GPU support.\n",
      "3.6.4 |Anaconda custom (64-bit)| (default, Jan 16 2018, 10:22:32) [MSC v.1900 64 bit (AMD64)] \n",
      "\n",
      "last run on 2018-11-09 14:50:00.700050\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import scipy.io\n",
    "import subprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow successfully installed.\")\n",
    "if tf.test.is_built_with_cuda():\n",
    "    print(\"The installed version of TensorFlow includes GPU support.\")\n",
    "print(sys.version, \"\\n\")\n",
    "now = datetime.now()\n",
    "print(\"last run on \" + str(now))\n",
    "\n",
    "# define directories\n",
    "baseDir = os.getcwd()\n",
    "dataDir = r'D:\\MothSimulations\\11c-AggressiveManeuver\\Qstore\\hws_am_con'\n",
    "figDir = r'D:\\Dropbox\\AcademiaDropbox\\mothMachineLearning_dataAndFigs\\Figs'\n",
    "dataOutput = r'D:\\Dropbox\\AcademiaDropbox\\mothMachineLearning_dataAndFigs\\DataOutput'\n",
    "savedModels = r'D:\\Dropbox\\AcademiaDropbox\\mothMachineLearning_dataAndFigs\\savedModels'\n",
    "\n",
    "if not os.path.exists(dataOutput):\n",
    "    os.mkdir(dataOutput)\n",
    "if not os.path.exists(savedModels):\n",
    "    os.mkdir(savedModels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>simNum</th>\n",
       "      <th>timestep</th>\n",
       "      <th>F</th>\n",
       "      <th>alpha</th>\n",
       "      <th>phi_0</th>\n",
       "      <th>phi_99</th>\n",
       "      <th>phi_dot_0</th>\n",
       "      <th>phi_dot_99</th>\n",
       "      <th>tau</th>\n",
       "      <th>theta_0</th>\n",
       "      <th>...</th>\n",
       "      <th>x_0</th>\n",
       "      <th>x_99</th>\n",
       "      <th>x_dot_0</th>\n",
       "      <th>x_dot_99</th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_99</th>\n",
       "      <th>y_dot_0</th>\n",
       "      <th>y_dot_99</th>\n",
       "      <th>Fx</th>\n",
       "      <th>Fy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38304.433962</td>\n",
       "      <td>5.340270</td>\n",
       "      <td>3.926991</td>\n",
       "      <td>3.869604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.674295</td>\n",
       "      <td>10963.268558</td>\n",
       "      <td>0.785398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.853314</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>683.734561</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.298536</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-133.752363</td>\n",
       "      <td>22501.211072</td>\n",
       "      <td>-30998.792903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>34264.536249</td>\n",
       "      <td>2.465501</td>\n",
       "      <td>3.948428</td>\n",
       "      <td>4.019544</td>\n",
       "      <td>2.009397</td>\n",
       "      <td>5.425513</td>\n",
       "      <td>72580.767201</td>\n",
       "      <td>0.768478</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019322</td>\n",
       "      <td>-6.079672</td>\n",
       "      <td>-0.145396</td>\n",
       "      <td>-606.129200</td>\n",
       "      <td>2.520087</td>\n",
       "      <td>6.366770</td>\n",
       "      <td>250.908304</td>\n",
       "      <td>120.273149</td>\n",
       "      <td>-26727.166452</td>\n",
       "      <td>21441.012519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12045.791872</td>\n",
       "      <td>4.573819</td>\n",
       "      <td>3.966608</td>\n",
       "      <td>3.933017</td>\n",
       "      <td>-0.124272</td>\n",
       "      <td>-2.775517</td>\n",
       "      <td>93435.855346</td>\n",
       "      <td>0.768515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.509507</td>\n",
       "      <td>2.986736</td>\n",
       "      <td>52.490438</td>\n",
       "      <td>193.914740</td>\n",
       "      <td>4.834518</td>\n",
       "      <td>2.470847</td>\n",
       "      <td>-18.848146</td>\n",
       "      <td>-211.050011</td>\n",
       "      <td>-1663.845687</td>\n",
       "      <td>-11930.327714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35562.854879</td>\n",
       "      <td>0.767089</td>\n",
       "      <td>4.000007</td>\n",
       "      <td>4.111777</td>\n",
       "      <td>3.434103</td>\n",
       "      <td>7.246049</td>\n",
       "      <td>-96088.338473</td>\n",
       "      <td>0.773977</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.836633</td>\n",
       "      <td>-4.334782</td>\n",
       "      <td>-187.046694</td>\n",
       "      <td>-157.064454</td>\n",
       "      <td>6.838940</td>\n",
       "      <td>17.508792</td>\n",
       "      <td>218.755312</td>\n",
       "      <td>844.589134</td>\n",
       "      <td>25602.909376</td>\n",
       "      <td>24682.132781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>20856.636628</td>\n",
       "      <td>2.424378</td>\n",
       "      <td>4.009324</td>\n",
       "      <td>3.995061</td>\n",
       "      <td>-2.309667</td>\n",
       "      <td>0.686804</td>\n",
       "      <td>-27142.805375</td>\n",
       "      <td>0.815904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429770</td>\n",
       "      <td>2.895126</td>\n",
       "      <td>313.730247</td>\n",
       "      <td>-65.350386</td>\n",
       "      <td>8.392823</td>\n",
       "      <td>6.771320</td>\n",
       "      <td>-58.222983</td>\n",
       "      <td>-102.458917</td>\n",
       "      <td>-15718.389244</td>\n",
       "      <td>13708.812166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   simNum  timestep             F     alpha     phi_0    phi_99  phi_dot_0  \\\n",
       "0       0         0  38304.433962  5.340270  3.926991  3.869604   0.000000   \n",
       "1       0         1  34264.536249  2.465501  3.948428  4.019544   2.009397   \n",
       "2       0         2  12045.791872  4.573819  3.966608  3.933017  -0.124272   \n",
       "3       0         3  35562.854879  0.767089  4.000007  4.111777   3.434103   \n",
       "4       0         4  20856.636628  2.424378  4.009324  3.995061  -2.309667   \n",
       "\n",
       "   phi_dot_99           tau   theta_0      ...            x_0      x_99  \\\n",
       "0   -5.674295  10963.268558  0.785398      ...       0.000000  6.853314   \n",
       "1    5.425513  72580.767201  0.768478      ...      -0.019322 -6.079672   \n",
       "2   -2.775517  93435.855346  0.768515      ...       0.509507  2.986736   \n",
       "3    7.246049 -96088.338473  0.773977      ...      -0.836633 -4.334782   \n",
       "4    0.686804 -27142.805375  0.815904      ...       0.429770  2.895126   \n",
       "\n",
       "      x_dot_0    x_dot_99       y_0       y_99     y_dot_0    y_dot_99  \\\n",
       "0    0.000100  683.734561  0.000000  -1.298536    0.000100 -133.752363   \n",
       "1   -0.145396 -606.129200  2.520087   6.366770  250.908304  120.273149   \n",
       "2   52.490438  193.914740  4.834518   2.470847  -18.848146 -211.050011   \n",
       "3 -187.046694 -157.064454  6.838940  17.508792  218.755312  844.589134   \n",
       "4  313.730247  -65.350386  8.392823   6.771320  -58.222983 -102.458917   \n",
       "\n",
       "             Fx            Fy  \n",
       "0  22501.211072 -30998.792903  \n",
       "1 -26727.166452  21441.012519  \n",
       "2  -1663.845687 -11930.327714  \n",
       "3  25602.909376  24682.132781  \n",
       "4 -15718.389244  13708.812166  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "trainDF2 = pd.read_csv(os.path.join(dataOutput, \"smallDF.csv\"))\n",
    "trainDF2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataset\n",
    "X = trainDF2.loc[:, [\"x_0\", \"y_0\", \"phi_0\", \"theta_0\", \n",
    "                    \"x_99\", \"y_99\", \"phi_99\", \"theta_99\", \n",
    "                   \"x_dot_0\", \"y_dot_0\", \"phi_dot_0\", \"theta_dot_0\"]]\n",
    "\n",
    "Y = trainDF2.loc[:, [\"Fx\", \"Fy\", \"tau\", \"x_dot_99\", \"y_dot_99\", \n",
    "                    \"phi_dot_99\", \"theta_dot_99\"] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_0</th>\n",
       "      <th>y_0</th>\n",
       "      <th>phi_0</th>\n",
       "      <th>theta_0</th>\n",
       "      <th>x_99</th>\n",
       "      <th>y_99</th>\n",
       "      <th>phi_99</th>\n",
       "      <th>theta_99</th>\n",
       "      <th>x_dot_0</th>\n",
       "      <th>y_dot_0</th>\n",
       "      <th>phi_dot_0</th>\n",
       "      <th>theta_dot_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.926991</td>\n",
       "      <td>0.785398</td>\n",
       "      <td>6.853314</td>\n",
       "      <td>-1.298536</td>\n",
       "      <td>3.869604</td>\n",
       "      <td>0.746382</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.019322</td>\n",
       "      <td>2.520087</td>\n",
       "      <td>3.948428</td>\n",
       "      <td>0.768478</td>\n",
       "      <td>-6.079672</td>\n",
       "      <td>6.366770</td>\n",
       "      <td>4.019544</td>\n",
       "      <td>0.940422</td>\n",
       "      <td>-0.145396</td>\n",
       "      <td>250.908304</td>\n",
       "      <td>2.009397</td>\n",
       "      <td>0.087406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.509507</td>\n",
       "      <td>4.834518</td>\n",
       "      <td>3.966608</td>\n",
       "      <td>0.768515</td>\n",
       "      <td>2.986736</td>\n",
       "      <td>2.470847</td>\n",
       "      <td>3.933017</td>\n",
       "      <td>0.868402</td>\n",
       "      <td>52.490438</td>\n",
       "      <td>-18.848146</td>\n",
       "      <td>-0.124272</td>\n",
       "      <td>-1.029996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.836633</td>\n",
       "      <td>6.838940</td>\n",
       "      <td>4.000007</td>\n",
       "      <td>0.773977</td>\n",
       "      <td>-4.334782</td>\n",
       "      <td>17.508792</td>\n",
       "      <td>4.111777</td>\n",
       "      <td>0.747358</td>\n",
       "      <td>-187.046694</td>\n",
       "      <td>218.755312</td>\n",
       "      <td>3.434103</td>\n",
       "      <td>2.037211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.429770</td>\n",
       "      <td>8.392823</td>\n",
       "      <td>4.009324</td>\n",
       "      <td>0.815904</td>\n",
       "      <td>2.895126</td>\n",
       "      <td>6.771320</td>\n",
       "      <td>3.995061</td>\n",
       "      <td>0.761962</td>\n",
       "      <td>313.730247</td>\n",
       "      <td>-58.222983</td>\n",
       "      <td>-2.309667</td>\n",
       "      <td>-0.673328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        x_0       y_0     phi_0   theta_0      x_99       y_99    phi_99  \\\n",
       "0  0.000000  0.000000  3.926991  0.785398  6.853314  -1.298536  3.869604   \n",
       "1 -0.019322  2.520087  3.948428  0.768478 -6.079672   6.366770  4.019544   \n",
       "2  0.509507  4.834518  3.966608  0.768515  2.986736   2.470847  3.933017   \n",
       "3 -0.836633  6.838940  4.000007  0.773977 -4.334782  17.508792  4.111777   \n",
       "4  0.429770  8.392823  4.009324  0.815904  2.895126   6.771320  3.995061   \n",
       "\n",
       "   theta_99     x_dot_0     y_dot_0  phi_dot_0  theta_dot_0  \n",
       "0  0.746382    0.000100    0.000100   0.000000     0.000000  \n",
       "1  0.940422   -0.145396  250.908304   2.009397     0.087406  \n",
       "2  0.868402   52.490438  -18.848146  -0.124272    -1.029996  \n",
       "3  0.747358 -187.046694  218.755312   3.434103     2.037211  \n",
       "4  0.761962  313.730247  -58.222983  -2.309667    -0.673328  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fx</th>\n",
       "      <th>Fy</th>\n",
       "      <th>tau</th>\n",
       "      <th>x_dot_99</th>\n",
       "      <th>y_dot_99</th>\n",
       "      <th>phi_dot_99</th>\n",
       "      <th>theta_dot_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22501.211072</td>\n",
       "      <td>-30998.792903</td>\n",
       "      <td>10963.268558</td>\n",
       "      <td>683.734561</td>\n",
       "      <td>-133.752363</td>\n",
       "      <td>-5.674295</td>\n",
       "      <td>-4.770986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-26727.166452</td>\n",
       "      <td>21441.012519</td>\n",
       "      <td>72580.767201</td>\n",
       "      <td>-606.129200</td>\n",
       "      <td>120.273149</td>\n",
       "      <td>5.425513</td>\n",
       "      <td>10.479995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1663.845687</td>\n",
       "      <td>-11930.327714</td>\n",
       "      <td>93435.855346</td>\n",
       "      <td>193.914740</td>\n",
       "      <td>-211.050011</td>\n",
       "      <td>-2.775517</td>\n",
       "      <td>3.897680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25602.909376</td>\n",
       "      <td>24682.132781</td>\n",
       "      <td>-96088.338473</td>\n",
       "      <td>-157.064454</td>\n",
       "      <td>844.589134</td>\n",
       "      <td>7.246049</td>\n",
       "      <td>0.318982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-15718.389244</td>\n",
       "      <td>13708.812166</td>\n",
       "      <td>-27142.805375</td>\n",
       "      <td>-65.350386</td>\n",
       "      <td>-102.458917</td>\n",
       "      <td>0.686804</td>\n",
       "      <td>-1.304712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Fx            Fy           tau    x_dot_99    y_dot_99  \\\n",
       "0  22501.211072 -30998.792903  10963.268558  683.734561 -133.752363   \n",
       "1 -26727.166452  21441.012519  72580.767201 -606.129200  120.273149   \n",
       "2  -1663.845687 -11930.327714  93435.855346  193.914740 -211.050011   \n",
       "3  25602.909376  24682.132781 -96088.338473 -157.064454  844.589134   \n",
       "4 -15718.389244  13708.812166 -27142.805375  -65.350386 -102.458917   \n",
       "\n",
       "   phi_dot_99  theta_dot_99  \n",
       "0   -5.674295     -4.770986  \n",
       "1    5.425513     10.479995  \n",
       "2   -2.775517      3.897680  \n",
       "3    7.246049      0.318982  \n",
       "4    0.686804     -1.304712  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test train split\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data \n",
    "scalerX = StandardScaler()  \n",
    "scalerY = StandardScaler()  \n",
    "\n",
    "# Don't cheat - fit only on training data\n",
    "scalerX.fit(Xtrain)  \n",
    "scalerY.fit(Ytrain) \n",
    "\n",
    "Xtrain_scaled = scalerX.transform(Xtrain)  \n",
    "Ytrain_scaled = scalerY.transform(Ytrain)  \n",
    "\n",
    "# apply same transformation to test data\n",
    "Xtest_scaled = scalerX.transform(Xtest)\n",
    "Ytest_scaled = scalerY.transform(Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "# Keras callcacks\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network\n",
    "def create_network(optimizer = 'rmsprop', dropout_rate=0.1):\n",
    "    NumHiddenLayers = 3\n",
    "    numUnits = 400\n",
    "    K.clear_session()\n",
    "    \n",
    "    inputs = Input(shape=(Xtrain_scaled.shape[1],))\n",
    "    x = Dense(numUnits, activation='tanh')(inputs)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    for ii in np.arange(1, NumHiddenLayers + 1):\n",
    "        x = Dense(numUnits, activation='tanh')(x)\n",
    "    predictions = Dense(Ytrain_scaled.shape[1], activation='linear')(x)\n",
    "\n",
    "    # create model\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(loss=\"mse\", optimizer = optimizer,   metrics = ['mse'])\n",
    "\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refref here: create dict of hyperparameters\n",
    "\n",
    "# Create hyperparameter space\n",
    "NumHiddenLayers = [4, 2, 8]\n",
    "numUnits  = [400, 200]\n",
    "epochs = [5, 10, 30]\n",
    "batches1 = [2**14, 2**10, 2**8] \n",
    "optimizers = ['rmsprop', 'adam']\n",
    "dropout_rate = [0.0, 0.2, 0.5]\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(optimizer=optimizers, \n",
    "                       epochs=epochs, \n",
    "                       batch_size=batches1) \n",
    "#                        dropout_rate = dropout_rate)  \n",
    "#                        numUnits = numUnits) \n",
    "#                       NumUnitsPerLayer = NumUnitsPerLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_scaled_small = Xtrain_scaled[0:8000, :]\n",
    "Ytrain_scaled_small = Ytrain_scaled[0:8000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV] optimizer=adam, epochs=10, batch_size=16384 .....................\n",
      "Epoch 1/10\n",
      " - 0s - loss: 1.0900 - mean_squared_error: 1.0900\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.7858 - mean_squared_error: 0.7858\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.5788 - mean_squared_error: 0.5788\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.4701 - mean_squared_error: 0.4701\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.4078 - mean_squared_error: 0.4078\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.3563 - mean_squared_error: 0.3563\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.3078 - mean_squared_error: 0.3078\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.2671 - mean_squared_error: 0.2671\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.2380 - mean_squared_error: 0.2380\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.2182 - mean_squared_error: 0.2182\n",
      "[CV] ...... optimizer=adam, epochs=10, batch_size=16384, total=   0.6s\n",
      "[CV] optimizer=adam, epochs=10, batch_size=16384 .....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 0s - loss: 1.0961 - mean_squared_error: 1.0961\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.7858 - mean_squared_error: 0.7858\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.5597 - mean_squared_error: 0.5597\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.4354 - mean_squared_error: 0.4354\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.3738 - mean_squared_error: 0.3738\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.3340 - mean_squared_error: 0.3340\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.2973 - mean_squared_error: 0.2973\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.2626 - mean_squared_error: 0.2626\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.2345 - mean_squared_error: 0.2345\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.2152 - mean_squared_error: 0.2152\n",
      "[CV] ...... optimizer=adam, epochs=10, batch_size=16384, total=   0.6s\n",
      "[CV] optimizer=adam, epochs=10, batch_size=16384 .....................\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.9382 - mean_squared_error: 0.9382\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.6718 - mean_squared_error: 0.6718\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.4900 - mean_squared_error: 0.4900\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.3941 - mean_squared_error: 0.3941\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.3409 - mean_squared_error: 0.3409\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.2998 - mean_squared_error: 0.2998\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.2628 - mean_squared_error: 0.2628\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.2322 - mean_squared_error: 0.2322\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.2090 - mean_squared_error: 0.2090\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.1893 - mean_squared_error: 0.1893\n",
      "[CV] ...... optimizer=adam, epochs=10, batch_size=16384, total=   0.6s\n",
      "[CV] optimizer=rmsprop, epochs=30, batch_size=256 ....................\n",
      "Epoch 1/30\n",
      " - 0s - loss: 0.2306 - mean_squared_error: 0.2306\n",
      "Epoch 2/30\n",
      " - 0s - loss: 0.0836 - mean_squared_error: 0.0836\n",
      "Epoch 3/30\n",
      " - 0s - loss: 0.0717 - mean_squared_error: 0.0717\n",
      "Epoch 4/30\n",
      " - 0s - loss: 0.0652 - mean_squared_error: 0.0652\n",
      "Epoch 5/30\n",
      " - 0s - loss: 0.0610 - mean_squared_error: 0.0610\n",
      "Epoch 6/30\n",
      " - 0s - loss: 0.0555 - mean_squared_error: 0.0555\n",
      "Epoch 7/30\n",
      " - 0s - loss: 0.0499 - mean_squared_error: 0.0499\n",
      "Epoch 8/30\n",
      " - 0s - loss: 0.0486 - mean_squared_error: 0.0486\n",
      "Epoch 9/30\n",
      " - 0s - loss: 0.0458 - mean_squared_error: 0.0458\n",
      "Epoch 10/30\n",
      " - 0s - loss: 0.0434 - mean_squared_error: 0.0434\n",
      "Epoch 11/30\n",
      " - 0s - loss: 0.0388 - mean_squared_error: 0.0388\n",
      "Epoch 12/30\n",
      " - 0s - loss: 0.0386 - mean_squared_error: 0.0386\n",
      "Epoch 13/30\n",
      " - 0s - loss: 0.0354 - mean_squared_error: 0.0354\n",
      "Epoch 14/30\n",
      " - 0s - loss: 0.0333 - mean_squared_error: 0.0333\n",
      "Epoch 15/30\n",
      " - 0s - loss: 0.0331 - mean_squared_error: 0.0331\n",
      "Epoch 16/30\n",
      " - 0s - loss: 0.0318 - mean_squared_error: 0.0318\n",
      "Epoch 17/30\n",
      " - 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 18/30\n",
      " - 0s - loss: 0.0284 - mean_squared_error: 0.0284\n",
      "Epoch 19/30\n",
      " - 0s - loss: 0.0267 - mean_squared_error: 0.0267\n",
      "Epoch 20/30\n",
      " - 0s - loss: 0.0249 - mean_squared_error: 0.0249\n",
      "Epoch 21/30\n",
      " - 0s - loss: 0.0238 - mean_squared_error: 0.0238\n",
      "Epoch 22/30\n",
      " - 0s - loss: 0.0225 - mean_squared_error: 0.0225\n",
      "Epoch 23/30\n",
      " - 0s - loss: 0.0215 - mean_squared_error: 0.0215\n",
      "Epoch 24/30\n",
      " - 0s - loss: 0.0211 - mean_squared_error: 0.0211\n",
      "Epoch 25/30\n",
      " - 0s - loss: 0.0196 - mean_squared_error: 0.0196\n",
      "Epoch 26/30\n",
      " - 0s - loss: 0.0179 - mean_squared_error: 0.0179\n",
      "Epoch 27/30\n",
      " - 0s - loss: 0.0162 - mean_squared_error: 0.0162\n",
      "Epoch 28/30\n",
      " - 0s - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Epoch 29/30\n",
      " - 0s - loss: 0.0158 - mean_squared_error: 0.0158\n",
      "Epoch 30/30\n",
      " - 0s - loss: 0.0148 - mean_squared_error: 0.0148\n",
      "[CV] ..... optimizer=rmsprop, epochs=30, batch_size=256, total=   2.3s\n",
      "[CV] optimizer=rmsprop, epochs=30, batch_size=256 ....................\n",
      "Epoch 1/30\n",
      " - 0s - loss: 0.2355 - mean_squared_error: 0.2355\n",
      "Epoch 2/30\n",
      " - 0s - loss: 0.0813 - mean_squared_error: 0.0813\n",
      "Epoch 3/30\n",
      " - 0s - loss: 0.0734 - mean_squared_error: 0.0734\n",
      "Epoch 4/30\n",
      " - 0s - loss: 0.0679 - mean_squared_error: 0.0679\n",
      "Epoch 5/30\n",
      " - 0s - loss: 0.0586 - mean_squared_error: 0.0586\n",
      "Epoch 6/30\n",
      " - 0s - loss: 0.0570 - mean_squared_error: 0.0570\n",
      "Epoch 7/30\n",
      " - 0s - loss: 0.0511 - mean_squared_error: 0.0511\n",
      "Epoch 8/30\n",
      " - 0s - loss: 0.0502 - mean_squared_error: 0.0502\n",
      "Epoch 9/30\n",
      " - 0s - loss: 0.0441 - mean_squared_error: 0.0441\n",
      "Epoch 10/30\n",
      " - 0s - loss: 0.0419 - mean_squared_error: 0.0419\n",
      "Epoch 11/30\n",
      " - 0s - loss: 0.0419 - mean_squared_error: 0.0419\n",
      "Epoch 12/30\n",
      " - 0s - loss: 0.0379 - mean_squared_error: 0.0379\n",
      "Epoch 13/30\n",
      " - 0s - loss: 0.0365 - mean_squared_error: 0.0365\n",
      "Epoch 14/30\n",
      " - 0s - loss: 0.0354 - mean_squared_error: 0.0354\n",
      "Epoch 15/30\n",
      " - 0s - loss: 0.0329 - mean_squared_error: 0.0329\n",
      "Epoch 16/30\n",
      " - 0s - loss: 0.0315 - mean_squared_error: 0.0315\n",
      "Epoch 17/30\n",
      " - 0s - loss: 0.0286 - mean_squared_error: 0.0286\n",
      "Epoch 18/30\n",
      " - 0s - loss: 0.0287 - mean_squared_error: 0.0287\n",
      "Epoch 19/30\n",
      " - 0s - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 20/30\n",
      " - 0s - loss: 0.0250 - mean_squared_error: 0.0250\n",
      "Epoch 21/30\n",
      " - 0s - loss: 0.0254 - mean_squared_error: 0.0254\n",
      "Epoch 22/30\n",
      " - 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 23/30\n",
      " - 0s - loss: 0.0230 - mean_squared_error: 0.0230\n",
      "Epoch 24/30\n",
      " - 0s - loss: 0.0211 - mean_squared_error: 0.0211\n",
      "Epoch 25/30\n",
      " - 0s - loss: 0.0189 - mean_squared_error: 0.0189\n",
      "Epoch 26/30\n",
      " - 0s - loss: 0.0187 - mean_squared_error: 0.0187\n",
      "Epoch 27/30\n",
      " - 0s - loss: 0.0175 - mean_squared_error: 0.0175\n",
      "Epoch 28/30\n",
      " - 0s - loss: 0.0164 - mean_squared_error: 0.0164\n",
      "Epoch 29/30\n",
      " - 0s - loss: 0.0158 - mean_squared_error: 0.0158\n",
      "Epoch 30/30\n",
      " - 0s - loss: 0.0157 - mean_squared_error: 0.0157\n",
      "[CV] ..... optimizer=rmsprop, epochs=30, batch_size=256, total=   2.4s\n",
      "[CV] optimizer=rmsprop, epochs=30, batch_size=256 ....................\n",
      "Epoch 1/30\n",
      " - 0s - loss: 0.2193 - mean_squared_error: 0.2193\n",
      "Epoch 2/30\n",
      " - 0s - loss: 0.0764 - mean_squared_error: 0.0764\n",
      "Epoch 3/30\n",
      " - 0s - loss: 0.0740 - mean_squared_error: 0.0740\n",
      "Epoch 4/30\n",
      " - 0s - loss: 0.0651 - mean_squared_error: 0.0651\n",
      "Epoch 5/30\n",
      " - 0s - loss: 0.0571 - mean_squared_error: 0.0571\n",
      "Epoch 6/30\n",
      " - 0s - loss: 0.0549 - mean_squared_error: 0.0549\n",
      "Epoch 7/30\n",
      " - 0s - loss: 0.0533 - mean_squared_error: 0.0533\n",
      "Epoch 8/30\n",
      " - 0s - loss: 0.0497 - mean_squared_error: 0.0497\n",
      "Epoch 9/30\n",
      " - 0s - loss: 0.0445 - mean_squared_error: 0.0445\n",
      "Epoch 10/30\n",
      " - 0s - loss: 0.0433 - mean_squared_error: 0.0433\n",
      "Epoch 11/30\n",
      " - 0s - loss: 0.0403 - mean_squared_error: 0.0403\n",
      "Epoch 12/30\n",
      " - 0s - loss: 0.0381 - mean_squared_error: 0.0381\n",
      "Epoch 13/30\n",
      " - 0s - loss: 0.0366 - mean_squared_error: 0.0366\n",
      "Epoch 14/30\n",
      " - 0s - loss: 0.0359 - mean_squared_error: 0.0359\n",
      "Epoch 15/30\n",
      " - 0s - loss: 0.0312 - mean_squared_error: 0.0312\n",
      "Epoch 16/30\n",
      " - 0s - loss: 0.0303 - mean_squared_error: 0.0303\n",
      "Epoch 17/30\n",
      " - 0s - loss: 0.0322 - mean_squared_error: 0.0322\n",
      "Epoch 18/30\n",
      " - 0s - loss: 0.0277 - mean_squared_error: 0.0277\n",
      "Epoch 19/30\n",
      " - 0s - loss: 0.0264 - mean_squared_error: 0.0264\n",
      "Epoch 20/30\n",
      " - 0s - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "Epoch 21/30\n",
      " - 0s - loss: 0.0237 - mean_squared_error: 0.0237\n",
      "Epoch 22/30\n",
      " - 0s - loss: 0.0221 - mean_squared_error: 0.0221\n",
      "Epoch 23/30\n",
      " - 0s - loss: 0.0216 - mean_squared_error: 0.0216\n",
      "Epoch 24/30\n",
      " - 0s - loss: 0.0201 - mean_squared_error: 0.0201\n",
      "Epoch 25/30\n",
      " - 0s - loss: 0.0191 - mean_squared_error: 0.0191\n",
      "Epoch 26/30\n",
      " - 0s - loss: 0.0192 - mean_squared_error: 0.0192\n",
      "Epoch 27/30\n",
      " - 0s - loss: 0.0166 - mean_squared_error: 0.0166\n",
      "Epoch 28/30\n",
      " - 0s - loss: 0.0155 - mean_squared_error: 0.0155\n",
      "Epoch 29/30\n",
      " - 0s - loss: 0.0150 - mean_squared_error: 0.0150\n",
      "Epoch 30/30\n",
      " - 0s - loss: 0.0150 - mean_squared_error: 0.0150\n",
      "[CV] ..... optimizer=rmsprop, epochs=30, batch_size=256, total=   2.3s\n",
      "[CV] optimizer=rmsprop, epochs=10, batch_size=256 ....................\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2237 - mean_squared_error: 0.2237\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.0779 - mean_squared_error: 0.0779\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0685 - mean_squared_error: 0.0685\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0663 - mean_squared_error: 0.0663\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0604 - mean_squared_error: 0.0604\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0551 - mean_squared_error: 0.0551\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0533 - mean_squared_error: 0.0533\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0469 - mean_squared_error: 0.0469\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0458 - mean_squared_error: 0.0458\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0432 - mean_squared_error: 0.0432\n",
      "[CV] ..... optimizer=rmsprop, epochs=10, batch_size=256, total=   0.9s\n",
      "[CV] optimizer=rmsprop, epochs=10, batch_size=256 ....................\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2349 - mean_squared_error: 0.2349\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.0842 - mean_squared_error: 0.0842\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0770 - mean_squared_error: 0.0770\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0681 - mean_squared_error: 0.0681\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0643 - mean_squared_error: 0.0643\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0560 - mean_squared_error: 0.0560\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0566 - mean_squared_error: 0.0566\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0491 - mean_squared_error: 0.0491\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0473 - mean_squared_error: 0.0473\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0432 - mean_squared_error: 0.0432\n",
      "[CV] ..... optimizer=rmsprop, epochs=10, batch_size=256, total=   1.1s\n",
      "[CV] optimizer=rmsprop, epochs=10, batch_size=256 ....................\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.2199 - mean_squared_error: 0.2199\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.0808 - mean_squared_error: 0.0808\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0722 - mean_squared_error: 0.0722\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0677 - mean_squared_error: 0.0677\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0611 - mean_squared_error: 0.0611\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0581 - mean_squared_error: 0.0581\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0516 - mean_squared_error: 0.0516\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0477 - mean_squared_error: 0.0477\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0461 - mean_squared_error: 0.0461\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0427 - mean_squared_error: 0.0427\n",
      "[CV] ..... optimizer=rmsprop, epochs=10, batch_size=256, total=   0.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:   12.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 0s - loss: 0.1781 - mean_squared_error: 0.1781\n",
      "Epoch 2/30\n",
      " - 0s - loss: 0.0707 - mean_squared_error: 0.0707\n",
      "Epoch 3/30\n",
      " - 0s - loss: 0.0648 - mean_squared_error: 0.0648\n",
      "Epoch 4/30\n",
      " - 0s - loss: 0.0563 - mean_squared_error: 0.0563\n",
      "Epoch 5/30\n",
      " - 0s - loss: 0.0508 - mean_squared_error: 0.0508\n",
      "Epoch 6/30\n",
      " - 0s - loss: 0.0451 - mean_squared_error: 0.0451\n",
      "Epoch 7/30\n",
      " - 0s - loss: 0.0429 - mean_squared_error: 0.0429\n",
      "Epoch 8/30\n",
      " - 0s - loss: 0.0385 - mean_squared_error: 0.0385\n",
      "Epoch 9/30\n",
      " - 0s - loss: 0.0360 - mean_squared_error: 0.0360\n",
      "Epoch 10/30\n",
      " - 0s - loss: 0.0320 - mean_squared_error: 0.0320\n",
      "Epoch 11/30\n",
      " - 0s - loss: 0.0302 - mean_squared_error: 0.0302\n",
      "Epoch 12/30\n",
      " - 0s - loss: 0.0280 - mean_squared_error: 0.0280\n",
      "Epoch 13/30\n",
      " - 0s - loss: 0.0259 - mean_squared_error: 0.0259\n",
      "Epoch 14/30\n",
      " - 0s - loss: 0.0240 - mean_squared_error: 0.0240\n",
      "Epoch 15/30\n",
      " - 0s - loss: 0.0224 - mean_squared_error: 0.0224\n",
      "Epoch 16/30\n",
      " - 0s - loss: 0.0209 - mean_squared_error: 0.0209\n",
      "Epoch 17/30\n",
      " - 0s - loss: 0.0186 - mean_squared_error: 0.0186\n",
      "Epoch 18/30\n",
      " - 0s - loss: 0.0173 - mean_squared_error: 0.0173\n",
      "Epoch 19/30\n",
      " - 0s - loss: 0.0157 - mean_squared_error: 0.0157\n",
      "Epoch 20/30\n",
      " - 0s - loss: 0.0146 - mean_squared_error: 0.0146\n",
      "Epoch 21/30\n",
      " - 0s - loss: 0.0134 - mean_squared_error: 0.0134\n",
      "Epoch 22/30\n",
      " - 0s - loss: 0.0128 - mean_squared_error: 0.0128\n",
      "Epoch 23/30\n",
      " - 0s - loss: 0.0116 - mean_squared_error: 0.0116\n",
      "Epoch 24/30\n",
      " - 0s - loss: 0.0116 - mean_squared_error: 0.0116\n",
      "Epoch 25/30\n",
      " - 0s - loss: 0.0104 - mean_squared_error: 0.0104\n",
      "Epoch 26/30\n",
      " - 0s - loss: 0.0099 - mean_squared_error: 0.0099\n",
      "Epoch 27/30\n",
      " - 0s - loss: 0.0100 - mean_squared_error: 0.0100\n",
      "Epoch 28/30\n",
      " - 0s - loss: 0.0096 - mean_squared_error: 0.0096\n",
      "Epoch 29/30\n",
      " - 0s - loss: 0.0086 - mean_squared_error: 0.0086\n",
      "Epoch 30/30\n",
      " - 0s - loss: 0.0088 - mean_squared_error: 0.0088\n"
     ]
    }
   ],
   "source": [
    "model = KerasRegressor(build_fn=create_network, verbose=1)\n",
    "\n",
    "\n",
    "# Create grid search\n",
    "grid = RandomizedSearchCV(estimator=neural_network, \n",
    "                          param_distributions=hyperparameters, \n",
    "                          n_iter = 3, verbose = 2, \n",
    "                         n_jobs=1)\n",
    "\n",
    "# Fit grid search\n",
    "grid_result = grid.fit(Xtrain_scaled_small, Ytrain_scaled_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 256, 'epochs': 5, 'optimizer': 'adam'}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the original model\n",
    "inputs = Input(shape=(Xtrain_scaled.shape[1],))\n",
    "x = Dense(400, activation='tanh')(inputs)\n",
    "x = Dense(400, activation='tanh')(x)\n",
    "x = Dense(400, activation='tanh')(x)\n",
    "x = Dense(16, activation='tanh')(x)\n",
    "predictions = Dense(Ytrain_scaled.shape[1], activation='linear')(x)\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics = ['mse'])\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_mean_squared_error', patience=50, \n",
    "                          verbose=1, mode='auto', min_delta = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "history = model.fit(Xtrain_scaled, Ytrain_scaled, validation_split = 0.3, \n",
    "                    verbose = 2, batch_size=2**14, epochs = 2, \n",
    "                    callbacks = [earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(model_history, saveFig = False):\n",
    "    fig, axs = plt.subplots(1,1,figsize=(10,5))\n",
    "    # summarize history for accuracy\n",
    "    axs.plot(range(1,len(model_history.history['mean_squared_error'])+1),\n",
    "             model_history.history['mean_squared_error'])\n",
    "    axs.plot(range(1,len(model_history.history['val_mean_squared_error'])+1),\n",
    "             model_history.history['val_mean_squared_error'])\n",
    "    axs.set_title('Model MSE')\n",
    "    axs.set_ylabel('mean_squared_error')\n",
    "    axs.set_xlabel('Epoch')\n",
    "    axs.set_xticks(np.arange(1,len(model_history.history['val_mean_squared_error'])+1),\n",
    "                   len(model_history.history['val_mean_squared_error'])/10)\n",
    "    axs.legend(['train', 'val'], loc='best')\n",
    "    if saveFig:\n",
    "        fig.savefig(os.path.join(figDir, \"ModelTraining.png\"), dpi = 120, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "plot_model_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model that was trained for much longer\n",
    "from keras.models import load_model\n",
    "model = load_model(os.path.join(savedModels, 'my_model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nonzero weights\n",
    "wts = model.get_weights().copy()\n",
    "nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "nzwts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show weights histograms\n",
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(2,5, figsize=np.array((15, 6)) , facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = 0.3, wspace=0.3)\n",
    "axs = axs.ravel(order = \"F\")\n",
    "\n",
    "for ii in range(int(len(wts) / 2)):  \n",
    "    jj= int(2*ii)\n",
    "    axs[jj].hist(wts[jj].reshape(-1), bins = 30)\n",
    "    axs[jj].set_xlabel(\"Layer \" + str(int(jj/2)) + \" weights\" + \", shape = \" + str(wts[jj].shape))\n",
    "    axs[jj+1].hist(wts[jj+1], bins = 30)\n",
    "    axs[jj+1].set_xlabel(\"Layer \" + str(int(jj/2)) + \" biases\" + \", shape = \" + str(wts[jj+1].shape))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set\n",
    "nnpreds = model.predict(Xtest_scaled[ :])\n",
    "\n",
    "# rescale\n",
    "nnpreds_unscaled = scalerY.inverse_transform(nnpreds)\n",
    "\n",
    "# show residuals\n",
    "# combine residual and regular plots\n",
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(2,7, figsize=np.array((30, 8)) / 1.7, facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = 0.1, wspace=0.5)\n",
    "axs = axs.ravel()\n",
    "\n",
    "# replace lightest colors with white\n",
    "import matplotlib.colors\n",
    "cmap = plt.cm.magma_r\n",
    "cmaplist = np.array([cmap(i) for i in range(cmap.N)])\n",
    "cmaplist[:,0:3] = np.divide(cmaplist[:, 0:3], 1.1)\n",
    "cmaplist[0] = (1,1,1,0.5)\n",
    "cmap = matplotlib.colors.LinearSegmentedColormap.from_list('mcm',cmaplist, cmap.N)\n",
    "\n",
    "for ii in range(len(Y.columns)):\n",
    "    \n",
    "    try:\n",
    "        axs[ii].hexbin(y = Ytest.iloc[:,ii],x = nnpreds_unscaled[:,ii], gridsize = 150, cmap = cmap)\n",
    "        axs[ii].yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "        axs[ii].ticklabel_format(style='sci',  axis='y', scilimits=(3,4))\n",
    "        axs[ii].axes.xaxis.set_ticklabels([])\n",
    "        if(ii == 0):\n",
    "            axs[ii].set_ylabel(\"Actual Value\")\n",
    "        axs[ii].set_title(nms2[ii])\n",
    "        axs[ii].plot(Ytest.iloc[0:1000,ii], Ytest.iloc[0:1000,ii], 'grey', linewidth = 1, linestyle  = \"--\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for ii in range(len(Y.columns)):\n",
    "    jj = ii + len(Y.columns)\n",
    "    \n",
    "    try:\n",
    "        axs[jj].hexbin(y = Ytest.iloc[:,ii] - nnpreds_unscaled[:,ii],x = nnpreds_unscaled[:,ii], gridsize = 150, cmap = cmap)\n",
    "        axs[jj].set_xlabel(\"Predicted Value\")\n",
    "        axs[jj].xaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "        axs[jj].yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "        mmin = np.min(Ytest.iloc[:,ii] - nnpreds_unscaled[:,ii])\n",
    "        mmax = np.max(Ytest.iloc[:,ii] - nnpreds_unscaled[:,ii])\n",
    "        \n",
    "        upper = np.max([np.abs(mmin), np.abs(mmax)])\n",
    "        axs[jj].set_ylim(-upper, upper)\n",
    "\n",
    "        if(ii == 0):\n",
    "            axs[jj].set_ylabel(\"Actual - Predicted\")\n",
    "        axs[jj].hlines(y = 0, xmin = np.min(nnpreds_unscaled[:,ii]), \n",
    "                       xmax = np.max(nnpreds_unscaled[:,ii]), linestyle =  \"--\", linewidth = 1)\n",
    "    except:\n",
    "        pass\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim distribution of weights -- cut out middle 20%\n",
    "for ii in np.arange(0, 7):\n",
    "    qants = np.percentile(np.reshape(wts[ii], -1), q = (40, 60), )\n",
    "    wts[ii][(wts[ii] > qants[0]) & (wts[ii] < qants[1])] = 0\n",
    "    \n",
    "# calculate number of nonzero weights\n",
    "nzwts = np.sum([np.nonzero(wts[ii].reshape(-1))[0].shape[0] for ii in range(len(wts))])\n",
    "nzwts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show new histogram of weights (excluding the 0's)\n",
    "# show weights histograms\n",
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(2,5, figsize=np.array((15, 6)) , facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = 0.3, wspace=0.3)\n",
    "axs = axs.ravel(order = \"F\")\n",
    "\n",
    "for ii in range(int(len(wts) / 2)):  \n",
    "    jj= int(2*ii)\n",
    "    \n",
    "    d1 = wts[jj].reshape(-1)\n",
    "    axs[jj].hist(d1[d1!=0], bins = 30, facecolor = '#d6bddb' )\n",
    "    axs[jj].set_xlabel(\"Layer \" + str(int(jj/2)) + \" weights\" + \", shape = \" + str(wts[jj].shape))\n",
    "\n",
    "    d2 = wts[jj+1]\n",
    "    axs[jj+1].hist(d2[d2!=0], bins = 30, facecolor = '#d6bddb')\n",
    "    axs[jj+1].set_xlabel(\"Layer \" + str(int(jj/2)) + \" biases\" + \", shape = \" + str(wts[jj+1].shape))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the validation.split is the last X% of the data\n",
    "int(0.3*Xtrain_scaled.shape[0])\n",
    "ValLoss = model.evaluate(Xtrain_scaled[-int(0.3*Xtrain_scaled.shape[0]):, :], \n",
    "                         Ytrain_scaled[-int(0.3*Xtrain_scaled.shape[0]):, :])\n",
    "print(ValLoss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set new weights and calculate new loss\n",
    "model.set_weights(wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValLoss = model.evaluate(Xtrain_scaled[-int(0.3*Xtrain_scaled.shape[0]):, :], \n",
    "                         Ytrain_scaled[-int(0.3*Xtrain_scaled.shape[0]):, :])\n",
    "print(ValLoss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set\n",
    "nnpreds = model.predict(Xtest_scaled[ :])\n",
    "\n",
    "# rescale\n",
    "nnpreds_unscaled = scalerY.inverse_transform(nnpreds)\n",
    "\n",
    "# show residuals\n",
    "# combine residual and regular plots\n",
    "plt.close(\"all\")\n",
    "fig, axs = plt.subplots(2,7, figsize=np.array((30, 8)) / 1.7, facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = 0.1, wspace=0.5)\n",
    "axs = axs.ravel()\n",
    "\n",
    "# replace lightest colors with white\n",
    "import matplotlib.colors\n",
    "cmap = plt.cm.magma_r\n",
    "cmaplist = np.array([cmap(i) for i in range(cmap.N)])\n",
    "cmaplist[:,0:3] = np.divide(cmaplist[:, 0:3], 1.1)\n",
    "cmaplist[0] = (1,1,1,0.5)\n",
    "cmap = matplotlib.colors.LinearSegmentedColormap.from_list('mcm',cmaplist, cmap.N)\n",
    "\n",
    "for ii in range(len(Y.columns)):\n",
    "    \n",
    "    try:\n",
    "        axs[ii].hexbin(y = Ytest.iloc[:,ii],x = nnpreds_unscaled[:,ii], gridsize = 150, cmap = cmap)\n",
    "        axs[ii].yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "        axs[ii].ticklabel_format(style='sci',  axis='y', scilimits=(3,4))\n",
    "        axs[ii].axes.xaxis.set_ticklabels([])\n",
    "        if(ii == 0):\n",
    "            axs[ii].set_ylabel(\"Actual Value\")\n",
    "        axs[ii].set_title(nms2[ii])\n",
    "        axs[ii].plot(Ytest.iloc[0:1000,ii], Ytest.iloc[0:1000,ii], 'grey', linewidth = 1, linestyle  = \"--\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for ii in range(len(Y.columns)):\n",
    "    jj = ii + len(Y.columns)\n",
    "    \n",
    "    try:\n",
    "        axs[jj].hexbin(y = Ytest.iloc[:,ii] - nnpreds_unscaled[:,ii],x = nnpreds_unscaled[:,ii], gridsize = 150, cmap = cmap)\n",
    "        axs[jj].set_xlabel(\"Predicted Value\")\n",
    "        axs[jj].xaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "        axs[jj].yaxis.set_major_formatter(ScalarFormatter(useMathText=True))\n",
    "        mmin = np.min(Ytest.iloc[:,ii] - nnpreds_unscaled[:,ii])\n",
    "        mmax = np.max(Ytest.iloc[:,ii] - nnpreds_unscaled[:,ii])\n",
    "        \n",
    "        upper = np.max([np.abs(mmin), np.abs(mmax)])\n",
    "        axs[jj].set_ylim(-upper, upper)\n",
    "\n",
    "        if(ii == 0):\n",
    "            axs[jj].set_ylabel(\"Actual - Predicted\")\n",
    "        axs[jj].hlines(y = 0, xmin = np.min(nnpreds_unscaled[:,ii]), \n",
    "                       xmax = np.max(nnpreds_unscaled[:,ii]), linestyle =  \"--\", linewidth = 1)\n",
    "    except:\n",
    "        pass\n",
    "plt.tight_layout()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
